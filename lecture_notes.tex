\documentclass[letterpaper,11pt,oneside,reqno]{amsart}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%bibliography
\usepackage[sorting=nyt,style=alphabetic,backend=bibtex,hyperref=true,doi=false,maxbibnames=9,maxcitenames=4]{biblatex}
\makeatletter
\def\blx@maxline{77}
\makeatother
\addbibresource{~/Dropbox/BiBTeX/bib.bib}
\sloppy

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}

%paper geometry
\usepackage[DIV=13]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\newcommand{\note}[1]{ {\color{blue}\textsf{(#1)}}}
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{MATH 3100 Spring 2022. Lecture summaries}

% OTHER AUTHORS 

\author{Leonid Petrov}

\date{}

\maketitle

\section{Lecture 1. January 19}

Introductory lecture, just saying hello to the students.

\section{Lecture 2. No deadline}

Section 1.1

A recording of an in-class piece, explaining 
the definition of the
probability space $\Omega$, events $\mathcal{F}$, 
and probability measure $P(A)$.

\section{Lecture 3. January 21}

Sections 1.1--1.2.

\begin{itemize}
	\item 
Recalling $(\Omega,\mathcal{F},P)$. 
\item 
Finite sample spaces, when it is 
enough to specify $P(\left\{ \omega \right\})$ for each singleton $\omega\in \Omega$.
We only need to have $P(\left\{ \omega \right\})\ge0$ and $\sum_{\omega\in \Omega}P\left( \left\{ \omega \right\} \right)=1$
\item Biased coin, 2 flips --- example
\item Equally likely outcomes, $P(A)=\frac{\# A}{\#\Omega}$. 
\item Example with urns.
\end{itemize}

\section{Lecture 4. January 24}

Section 1.2. Discussing 3 settings,
when we sample $k$ times from $n$ objects. This is an instance of equally likely
outcomes.
Recall the factorial:
\begin{equation*}
	\begin{cases}
		n!=1\cdot 2\cdot 3\cdot \ldots\cdot n,&n\ge1 
		\\
		0!=1.
	\end{cases}
\end{equation*}
So, $0!=1$, $1!=1$, $2!=2$, $3!=6$, $4!=24$, and so on.

\begin{itemize}
	\item Sampling with replacement, order matters: $\# \Omega_k=n^k$.
	\item Sampling without replacement, order matters: $\# \Omega_k=n(n-1)\ldots (n-k+1)=\dfrac{n!}{(n-k)!}$.
	\item Sampling without replacement, order does not matter:
		$$\# \Omega_k=\dfrac{n(n-1)\ldots(n-k+1) }{k!}
		=\dfrac{n!}{k!\, (n-k!)}=\binom nk .$$
		The quantity $\binom nk$ has a special name, ``binomial coefficient''.
\end{itemize}

\section{Lecture 5. January 26}

Section 1.3.
\begin{itemize}
	\item Probability space for rolling a die until you see a ``6''
	\item How to sum the geometric progression
	\item Continuous sample spaces, $[0,1]$ with length measure
	\item How to draw a random line in the plane (one example)
\end{itemize}

\section{Lecture 6. January 28}

Section 1.4. Rules of probability (theory only).
\begin{itemize}
	\item Venn diagrams. Probability ``behaves like the area of a Venn diagram''
	\item Decomposing an event
	\item Complements, also $P(A)=P(AB)+P(AB^c)$
	\item Monotonicity of probability
	\item Inclusion-exclusion
\end{itemize}

\section{Lecture 7. January 31}
Section 1.4. Examples on rules of probability --- inclusion/exclusion principle; helpful passing to 
the complement event
$A^c$.

A first look on random variables: if we flip a fair coin 5 times, and let $Y$ be the number of Heads, 
what is the probability distribution of $Y$? This is described in terms of the probability mass function,
which in this case takes the form
\begin{equation*}
	P(Y=0)=\frac{1}{32},\quad
	P(Y=1)=\frac{5}{32},\quad
	P(Y=2)=\frac{10}{32},\quad
	P(Y=3)=\frac{10}{32},\quad
	P(Y=4)=\frac{5}{32},\quad
	P(Y=5)=\frac{1}{32}.
\end{equation*}

\section{Lecture 8. Feb 2}

Section 1.5. Random variables. 
\begin{itemize}
	\item Definition of a random variable
	\item Splitting of $\Omega$ into level sets
	\item Discrete, continuous, and other random variables
	\item Examples of discrete random variables
	\item Probability mass function (pmf)
	\item Distribution of a random variable, example of 
		two random variables with the same distribution
		which are not the same.
\end{itemize}

\section{Lecture 9. February 4}

Sections 2.1, 2.2. Conditional probabilities, Bayes rule.

\begin{itemize}
	\item Conditional probability --- motivating example
	\item Definition of $P(A\mid B)$
	\item Chain product of conditional probabilities
	\item Total probability formula (averaged conditional probabilities)
	\item Bayes rule
	\item An example, testing for rare diseases
\end{itemize}


\section{Lecture 10. February 7}

Section 2.3. Independence.
\begin{itemize}
	\item Recall Bayes rule. One more example.
	\item Independence of two events.
	\item 
		Independence of $A,B$ is equivalent to the independence
		of $A^c, B$ and of $A^c,B^c$.
	\item Examples 2.18, 2.21
	\item Independence of multiple events means we need lots of product rules.
	\item Independence of random variables. Coin flips. Digits in 
		uniformly random $\omega\in [0,1]$.
\end{itemize}

\section{Lecture 11. February 11}

Sections 3.1, 3.2. 
\begin{itemize}
\item 
Discrete random variables. 
They are determined through pmf. 
Pmf properties (2) - nonnegativity, sum to one. Probability through pmf as a sum.
\item 
	Continuous random variables, in the same fashion. Probability density function. Properties of pdf (2); Probability through integrals.
\item 
	Cumulative distribution function (cdf).
	\begin{itemize}
		\item 
	definition
\item 
	how it looks for continuous r.v.; connection to pdf	
\item 
	how it looks for discrete r.v.
	\end{itemize}
	Some examples.
\end{itemize}

\section{Lecture 11.1. February 11}

This is an example of finding cdfs of random variables $(X,Y)$ which are coordinates
of a uniformly random point thrown into some figure in the plane.

\section{Lecture 12. February 14}

Section 3.3. Expectation.

\begin{itemize}
	\item 
Expectation - discrete rv

\item 
Expectation - continuous rv
\item 
Example - Bernoulli; binomial without proof
\item 
Example - geometric
\item 
Derivative technique
\item 
Use derivatives to get expectation of geometric
\item 
	Application to binomial distribution.
\item One more example of the computation of expectation for a continuous random variable.
\end{itemize}




\end{document}
