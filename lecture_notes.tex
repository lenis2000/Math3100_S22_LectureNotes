\documentclass[letterpaper,11pt,oneside,reqno]{amsart}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%bibliography
\usepackage[sorting=nyt,style=alphabetic,backend=bibtex,hyperref=true,doi=false,maxbibnames=9,maxcitenames=4]{biblatex}
\makeatletter
\def\blx@maxline{77}
\makeatother
\addbibresource{~/Dropbox/BiBTeX/bib.bib}
\sloppy

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%main packages
\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=red]{hyperref}
\usepackage{graphicx,color}
\usepackage{upgreek}
\usepackage[mathscr]{euscript}

%equations
\allowdisplaybreaks
\numberwithin{equation}{section}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.markings}

%conveniences
\usepackage{array}
\usepackage{adjustbox}
\usepackage{cleveref}
\usepackage{enumerate}

%paper geometry
\usepackage[DIV=13]{typearea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%draft-specific
\newcommand{\note}[1]{ {\color{blue}\textsf{(#1)}}}
\synctex=1
% \usepackage{refcheck,comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%this paper specific

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{theorem}[proposition]{Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{remark}[proposition]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{MATH 3100 Spring 2022. Lecture summaries}

% OTHER AUTHORS 

\author{Leonid Petrov}

\date{}

\maketitle

\section*{Lecture 1. January 19}

Introductory lecture, just saying hello to the students.

\section*{Lecture 2. No deadline}

Section 1.1

A recording of an in-class piece, explaining 
the definition of the
probability space $\Omega$, events $\mathcal{F}$, 
and probability measure $P(A)$.

\section*{Lecture 3. January 21}

Sections 1.1--1.2.

\begin{itemize}
	\item 
Recalling $(\Omega,\mathcal{F},P)$. 
\item 
Finite sample spaces, when it is 
enough to specify $P(\left\{ \omega \right\})$ for each singleton $\omega\in \Omega$.
We only need to have $P(\left\{ \omega \right\})\ge0$ and $\sum_{\omega\in \Omega}P\left( \left\{ \omega \right\} \right)=1$
\item Biased coin, 2 flips --- example
\item Equally likely outcomes, $P(A)=\frac{\# A}{\#\Omega}$. 
\item Example with urns.
\end{itemize}

\section*{Lecture 4. January 24}

Section 1.2. Discussing 3 settings,
when we sample $k$ times from $n$ objects. This is an instance of equally likely
outcomes.
Recall the factorial:
\begin{equation*}
	\begin{cases}
		n!=1\cdot 2\cdot 3\cdot \ldots\cdot n,&n\ge1 
		\\
		0!=1.
	\end{cases}
\end{equation*}
So, $0!=1$, $1!=1$, $2!=2$, $3!=6$, $4!=24$, and so on.

\begin{itemize}
	\item Sampling with replacement, order matters: $\# \Omega_k=n^k$.
	\item Sampling without replacement, order matters: $\# \Omega_k=n(n-1)\ldots (n-k+1)=\dfrac{n!}{(n-k)!}$.
	\item Sampling without replacement, order does not matter:
		$$\# \Omega_k=\dfrac{n(n-1)\ldots(n-k+1) }{k!}
		=\dfrac{n!}{k!\, (n-k!)}=\binom nk .$$
		The quantity $\binom nk$ has a special name, ``binomial coefficient''.
\end{itemize}

\section*{Lecture 5. January 26}

Section 1.3.
\begin{itemize}
	\item Probability space for rolling a die until you see a ``6''
	\item How to sum the geometric progression
	\item Continuous sample spaces, $[0,1]$ with length measure
	\item How to draw a random line in the plane (one example)
\end{itemize}

\section*{Lecture 6. January 28}

Section 1.4. Rules of probability (theory only).
\begin{itemize}
	\item Venn diagrams. Probability ``behaves like the area of a Venn diagram''
	\item Decomposing an event
	\item Complements, also $P(A)=P(AB)+P(AB^c)$
	\item Monotonicity of probability
	\item Inclusion-exclusion
\end{itemize}

\section*{Lecture 7. January 31}
Section 1.4. Examples on rules of probability --- inclusion/exclusion principle; helpful passing to 
the complement event
$A^c$.

A first look on random variables: if we flip a fair coin 5 times, and let $Y$ be the number of Heads, 
what is the probability distribution of $Y$? This is described in terms of the probability mass function,
which in this case takes the form
\begin{equation*}
	P(Y=0)=\frac{1}{32},\quad
	P(Y=1)=\frac{5}{32},\quad
	P(Y=2)=\frac{10}{32},\quad
	P(Y=3)=\frac{10}{32},\quad
	P(Y=4)=\frac{5}{32},\quad
	P(Y=5)=\frac{1}{32}.
\end{equation*}

\section*{Lecture 8. Feb 2}

Section 1.5. Random variables. 
\begin{itemize}
	\item Definition of a random variable
	\item Splitting of $\Omega$ into level sets
	\item Discrete, continuous, and other random variables
	\item Examples of discrete random variables
	\item Probability mass function (pmf)
	\item Distribution of a random variable, example of 
		two random variables with the same distribution
		which are not the same.
\end{itemize}

\section*{Lecture 9. February 4}

Sections 2.1, 2.2. Conditional probabilities, Bayes rule.

\begin{itemize}
	\item Conditional probability --- motivating example
	\item Definition of $P(A\mid B)$
	\item Chain product of conditional probabilities
	\item Total probability formula (averaged conditional probabilities)
	\item Bayes rule
	\item An example, testing for rare diseases
\end{itemize}


\section*{Lecture 10. February 7}

Section 2.3. Independence.
\begin{itemize}
	\item Recall Bayes rule. One more example.
	\item Independence of two events.
	\item 
		Independence of $A,B$ is equivalent to the independence
		of $A^c, B$ and of $A^c,B^c$.
	\item Examples 2.18, 2.21
	\item Independence of multiple events means we need lots of product rules.
	\item Independence of random variables. Coin flips. Digits in 
		uniformly random $\omega\in [0,1]$.
\end{itemize}

\section*{Lecture 11. February 11}

Sections 3.1, 3.2. 
\begin{itemize}
\item 
Discrete random variables. 
They are determined through pmf. 
Pmf properties (2) - nonnegativity, sum to one. Probability through pmf as a sum.
\item 
	Continuous random variables, in the same fashion. Probability density function. Properties of pdf (2); Probability through integrals.
\item 
	Cumulative distribution function (cdf).
	\begin{itemize}
		\item 
	definition
\item 
	how it looks for continuous r.v.; connection to pdf	
\item 
	how it looks for discrete r.v.
	\end{itemize}
	Some examples.
\end{itemize}

\section*{Lecture 11.1. February 11}

This is an example of finding cdfs of random variables $(X,Y)$ which are coordinates
of a uniformly random point thrown into some figure in the plane.

\section*{Lecture 12. February 14}

Section 3.3. Expectation.

\begin{itemize}
	\item 
Expectation - discrete rv

\item 
Expectation - continuous rv
\item 
Example - Bernoulli; binomial without proof
\item 
Example - geometric
\item 
Derivative technique
\item 
Use derivatives to get expectation of geometric
\item 
	Application to binomial distribution.
\item One more example of the computation of expectation for a continuous random variable.
\end{itemize}

\section*{Lecture 13. February 21}

Sections 3.1--3.4. Expectation, variance, derivative method.
\begin{itemize}
	\item Expectation of a function of a random variable. Discussion and definitions for 
		discrete and continuous cases.
	\item Variance. Definition, discussion. 
	\item Two formulas for the variance. If $E(X)=\mu$, then
		\begin{equation*}
			Var(X)=E\left( (X-\mu)^2 \right)=E\left( X^2 \right)-\mu^2.
		\end{equation*}
		The first formula explains why variance is nonnegative, and the 
		second formula is more practical for computation of the variance. 
	\item Discussion of the derivative method. Application to compute
		\begin{equation*}
			E(X),\qquad E\left( \frac{1}{1+X} \right)
		\end{equation*}
		for the Poisson random variable, where $p_X(k)=e^{-\lambda}\dfrac{\lambda^k}{k!}$, $k=0,1,2,\ldots $.
\end{itemize}


\section*{Lecture 14. February 25}

Sections 3.5, 4.1. 
Normal (=~Gaussian) distribution, normal approximation.
\begin{itemize}
	\item (neat thing) Expectation through cdf $F_X(x)$
	\item Linearity of $E$, transformations of $Var$
	\item Normal distribution (standard)
	\item Normal table
	\item Normalization, general normal distribution
	\item Approximation of the binomial distribution --- Central Limit Theorem (CLT)
	\item On the proof of CLT
\end{itemize}

\section*{Lecture 15 part 1. March 2}

Sections 4.1, 4.2.
\begin{itemize}
	\item Recall Central Limit Theorem. 
	\item Law of Large Numbers. Discussion: frequency interpretation of probability.
	\item Law of Large Numbers. Proof from Central Limit Theorem.
\end{itemize}

\section*{Lecture 15 part 2. March 2}

Section 4.4 (beginning). Poisson approximation of the binomial distribution
--- computation of the limit of 
$P(S_n=k)$ as $n\to+\infty$, $p\to0$, $np\to\lambda$ (where $\lambda>0$ is a fixed real
number), and $k$ is fixed.

\section*{Lecture 16. March 4}

Section 4.4 and 4.5. Poisson random variable, exponential random variable.
Their pmf (Poisson) and pdf (exponential); 
expectation, and variance. 

If $X\sim \mathrm{Poiss}(\lambda)$ and $T\sim \mathrm{Exp}(\lambda)$, then
\begin{equation*}
	E(X)=Var(X)=\lambda,\qquad E(T)=\frac{1}{\lambda},\qquad Var(T)=\frac{1}{\lambda^2}.
\end{equation*}

\section*{Lecture 17. March 14}

Sections 4.4--4.6. Poisson process from coin flipping.
\begin{itemize}
	\item Two descriptions of the coin-flipping process: through number of heads having
		binomial distribution; and through inter-heads intervals which
		have geometric distribution.
	\item Limit as $n\to\infty$ and $p=\lambda/n$. Geometric
		distribution becomes exponential, and 
		binomial distribution becomes Poisson.
	\item The whole coin-flipping process in the scaling limit
		turns into a new device, the Poisson process. 
		Poisson process is a random collection of points on the 
		nonnegative real line, with Poisson and exponential distributions
		built into its definition.
\end{itemize}

\section*{Lecture 18. March 16}

Sections 4.5--4.6 and 7.3. Poisson processes I.
\begin{itemize}
	\item Poisson process --- definition through point-count random variables
		$N_A$.
	\item Extracting exponential distributions from 
		Poisson process.
\end{itemize}

\section*{Lecture 19. March 18}

Sections 4.5--4.6 and 7.3. Poisson process II.
\begin{itemize}
	\item From Poisson process to gamma distributions.
	\item Emergence of uniform and binomial distributions from Poisson process.
	\item Remark about Poisson process in space.
\end{itemize}

\section*{Lecture 20. March 21}

Section 5.2 --- distribution of a function of a random variable.

\section*{Lecture 20. March 21. Part 2}

Queuing systems --- stochastic modeling via Poisson processes.

\section*{Lecture 21. March 23}

Joint distributions of random variables. 
Beginning chapter 6 in the textbook.
\begin{itemize}
	\item Joint pdf of two random variables
	\item Independence in terms of joint pdf
	\item $P(a<X\le b,c<Y\le d)=\int_a^b\int_c^d f_{X,Y}(x,y)dx dy$
	\item Application to a joint distribution of $(T,W)$ in a Poisson process
		(where $T$ is the time between bus 1 and bus 3, 
		and $W$ is the time between bus 3 and bus 4)
	\item Discrete pmf
	\item Marginal pmfs
	\item Independence in terms of pmfs
\end{itemize}

\section*{Lecture 22. March 25}

Joint distributions, discrete and continuous. Chapter 6 of the textbook. 
\begin{itemize}
	\item Main formulas about discrete and continuous joint distributions:
		\begin{enumerate}
			\item Integrate to 1
			\item Marginal distributions
			\item Probabilities of events
			\item Expectation of a function of a random variable
		\end{enumerate}
	\item Double integral via iterated integrals, general algorithm. 
	\item Double integral via iterated integrals --- example of a triangle.
\end{itemize}


\section*{Lecture 23. April 1}

Joint distributions and independence. Section 7.1
\begin{itemize}
\item Independent random variables, how does pmf and pdf look like
\item Example
\item pmf of X+Y - convolution of discrete distributions
\item example with Poisson (which also follows from Poisson process)
\item pdf of X+Y - convolution of continuous distributions
\item example with sum of uniform, and sum of exponentials
\end{itemize}

\section*{Lecture 24. April 4}

Convolution of normals. Indicators.
\begin{itemize}
	\item Sum of independent normal random variables
		is always normal. Proof for the case of equal variances
	\item Indicator random variables. Definition.
	\item Indicator random variables. Main property:
		$E(I_A+I_B)=E(I_A)+E(I_B)$
	\item An example: solving a problem using indicators.
\end{itemize}


\section*{Lecture 26. April 6}

Linearity of expectation. Section 8.1
\begin{itemize}
	\item Proof of linearity of expectation for joint pdfs
	\item Proof of linearity of expectation for joint pmf (both random variables discrete), 
		short discussion
	\item Proof of linearity of expectation using indicators and simple random variables, using approximation.
\end{itemize}


\section*{Lecture 26. April 8}

Expectation, covariance, variance, and independence
\begin{itemize}
	\item Product rule for expectations
	\item Counterexample for product rule for expectations
	\item Covariance (definition)
	\item Variance of the sum of independent random variables
	\item Application to binomial distribution.
\end{itemize}

\section*{Lecture 27. April 11}

Covariance and correlation (Section 8.4). Markov inequality (Section 9.1).
\begin{itemize}
	\item Example, covariance of $I_A,I_B$.
	\item Bilinearity of covariance.
	\item Cauchy--Schwartz inequality
		\begin{equation*}
			|Cov(X,Y)|\le\sqrt{Var(X)\cdot Var(Y)}
		\end{equation*}
	\item Correlation coefficient
	\item Markov inequality: For $X\ge0$ and $a>0$, we have
		\begin{equation*}
			P(X\ge a)\le \frac{E(X)}{a}.
		\end{equation*}
		
\end{itemize}





\end{document}
